# config/evaluation/evaluation.yaml
# Model configuration
model_path: "meta-llama/Meta-Llama-3-8B" # "meta-llama/Llama-3.1-8B-Instruct"  # Base model path or HuggingFace ID
adapter_path: "chrisjcc/Meta-Llama-3.1-8B-text2sql-adapter"  # Set to checkpoint path for fine-tuned adapter model
# Examples:
#   adapter_path: "${training.output_dir}/checkpoint-500"
#   adapter_path: "./outputs/code-llama-3-1-8b-text-to-sql/checkpoint-1000"  # Example: local adapter
#   adapter_path: "your-username/text2sql-adapter"  # Example: HuggingFace Hub adapter

# Evaluation dataset
num_eval_samples: 1000  # Number of samples to evaluate (use smaller for quick tests)

# Generation parameters
batch_size: 8  # Adjust based on GPU memory
temperature: 0.0  # 0.0 = greedy decoding (deterministic), >0.0 = sampling
max_new_tokens: 128  # Maximum number of tokens to generate

# Evaluation behavior
skip_baseline: false  # Set to true to skip baseline evaluation (faster)
num_examples: 3  # Number of example predictions to display
# Save all predictions to results file
save_predictions: true  # Save detailed predictions to JSON
